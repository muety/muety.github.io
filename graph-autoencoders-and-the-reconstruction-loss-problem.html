<!DOCTYPE html>
<html lang=en>
<head>
    <!-- so meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
    <meta name="referrer" content="no-referrer">
    <meta name="robots" content="noarchive">
    <meta name="description" content="ForewordThis article is meant to give a brief introduction into a prominent problem in the context of generative graph neural network (GNN) models and a few examples of different approaches to defeat">
<meta property="og:type" content="article">
<meta property="og:title" content="Graph Autoencoders and the Reconstruction Loss Problem">
<meta property="og:url" content="https://muetsch.io/graph-autoencoders-and-the-reconstruction-loss-problem.html">
<meta property="og:site_name" content="Ferdinand Mütsch">
<meta property="og:description" content="ForewordThis article is meant to give a brief introduction into a prominent problem in the context of generative graph neural network (GNN) models and a few examples of different approaches to defeat">
<meta property="og:locale">
<meta property="og:image" content="https://muetsch.io/images/gae_isomorphic_graphs.svg">
<meta property="og:image" content="https://muetsch.io/images/gae_permutation_invariance.webp">
<meta property="og:image" content="https://muetsch.io/images/gae_node_graph_level.svg">
<meta property="og:image" content="https://muetsch.io/images/vgae.svg">
<meta property="og:image" content="https://muetsch.io/images/condgen.webp">
<meta property="og:image" content="https://muetsch.io/images/pigvae.webp">
<meta property="article:published_time" content="2025-04-14T07:07:10.000Z">
<meta property="article:modified_time" content="2025-06-09T18:05:45.981Z">
<meta property="article:author" content="Ferdinand Mütsch">
<meta property="article:tag" content="ai">
<meta property="article:tag" content="machine-learning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://muetsch.io/images/gae_isomorphic_graphs.svg">
    
    
        
          
              <link rel="shortcut icon" href="/images/favicon.png">
          
        
        
          
            <link rel="icon" type="image/png" href="/images/favicon.png" sizes="192x192">
          
        
        
    
    <!-- title -->
    
    <title>Graph Autoencoders and the Reconstruction Loss Problem  | Ferdinand Mütsch</title>
    <!-- styles -->
    
<link rel="stylesheet" href="/css/style.css">

    <!-- rss -->
    
    

    <!-- rel-me links -->
    
      
        <link href="http://github.com/muety" rel="me">
      
        <link href="https://social.tchncs.de/@ferdi" rel="me">
      
        <link href="mailto:ferdinand@muetsch.io" rel="me">
      
    

    <!-- Webmention link -->
    
      <link href="https://webmention.io/muetsch.io/webmention" rel="webmention">
    

    
      <link href="https://webmention.io/muetsch.io/xmlrpc" rel="pingback">
    
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="/vendor/katex.min.css">

<link rel="stylesheet" href="/vendor/math.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.3.0"><link rel="alternate" href="/rss2.xml" title="Ferdinand Mütsch" type="application/rss+xml">
</head>

<body>
    
      <div id="header-post">
  <a id="menu-icon" href="#"><i class="fas fa-bars fa-lg"></i></a>
  <a id="menu-icon-tablet" href="#"><i class="fas fa-bars fa-lg"></i></a>
  <a id="top-icon-tablet" href="#" onclick="window.scrollTo(0, 0)" style="display:none;"><i class="fas fa-chevron-up fa-lg"></i></a>
  <span id="menu">
    <span id="nav">
      <ul>
         
          <li><a href="/">Home</a></li>
         
          <li><a href="/about/">About</a></li>
         
          <li><a href="/archives/">Blog</a></li>
         
          <li><a href="/reads/">Reads</a></li>
         
          <li><a href="/imprint/">Imprint</a></li>
        
      </ul>
    </span>
    <br/>
    <span id="actions">
      <ul>
        
        <li><a class="icon" href="/cycling-around-lake-constance.html"><i class="fas fa-chevron-left" aria-hidden="true" onmouseover='toggleById("i-prev");' onmouseout='toggleById("i-prev");'></i></a></li>
        
        
        <li><a class="icon" href="/running-fcos3d-monocular-3d-detection-on-custom-images.html"><i class="fas fa-chevron-right" aria-hidden="true" onmouseover='toggleById("i-next").toggleById();' onmouseout='toggleById("i-next");'></i></a></li>
        
        <li><a class="icon" href="#" onclick="window.scrollTo(0,0)"><i class="fas fa-chevron-up" aria-hidden="true" onmouseover='toggleById("i-top");' onmouseout='toggleById("i-top");'></i></a></li>
        <li><a class="icon" href="#"><i class="fas fa-share-alt" aria-hidden="true" onmouseover='toggleById("i-share");' onmouseout='toggleById("i-share");' onclick='toggleById("share");return false;'></i></a></li>
      </ul>
      <span id="i-prev" class="info" style="display:none;">Previous post</span>
      <span id="i-next" class="info" style="display:none;">Next post</span>
      <span id="i-top" class="info" style="display:none;">Back to top</span>
      <span id="i-share" class="info" style="display:none;">Share post</span>
    </span>
    <br/>
    <div id="share" style="display: none">
      <ul>
  <li><a class="icon" href="http://www.facebook.com/sharer.php?u=https://muetsch.io/graph-autoencoders-and-the-reconstruction-loss-problem.html"><i class="fas fa-facebook " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://twitter.com/share?url=https://muetsch.io/graph-autoencoders-and-the-reconstruction-loss-problem.html&text=Graph Autoencoders and the Reconstruction Loss Problem"><i class="fas fa-twitter " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.linkedin.com/shareArticle?url=https://muetsch.io/graph-autoencoders-and-the-reconstruction-loss-problem.html&title=Graph Autoencoders and the Reconstruction Loss Problem"><i class="fas fa-linkedin " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://pinterest.com/pin/create/bookmarklet/?url=https://muetsch.io/graph-autoencoders-and-the-reconstruction-loss-problem.html&is_video=false&description=Graph Autoencoders and the Reconstruction Loss Problem"><i class="fas fa-pinterest " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=Graph Autoencoders and the Reconstruction Loss Problem&body=Check out this article: https://muetsch.io/graph-autoencoders-and-the-reconstruction-loss-problem.html"><i class="fas fa-envelope " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://getpocket.com/save?url=https://muetsch.io/graph-autoencoders-and-the-reconstruction-loss-problem.html&title=Graph Autoencoders and the Reconstruction Loss Problem"><i class="fas fa-get-pocket " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://reddit.com/submit?url=https://muetsch.io/graph-autoencoders-and-the-reconstruction-loss-problem.html&title=Graph Autoencoders and the Reconstruction Loss Problem"><i class="fas fa-reddit " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.stumbleupon.com/submit?url=https://muetsch.io/graph-autoencoders-and-the-reconstruction-loss-problem.html&title=Graph Autoencoders and the Reconstruction Loss Problem"><i class="fas fa-stumbleupon " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://digg.com/submit?url=https://muetsch.io/graph-autoencoders-and-the-reconstruction-loss-problem.html&title=Graph Autoencoders and the Reconstruction Loss Problem"><i class="fas fa-digg " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.tumblr.com/share/link?url=https://muetsch.io/graph-autoencoders-and-the-reconstruction-loss-problem.html&name=Graph Autoencoders and the Reconstruction Loss Problem&description="><i class="fas fa-tumblr " aria-hidden="true"></i></a></li>
</ul>

    </div>
    <div id="toc">
      <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Foreword"><span class="toc-number">1.</span> <span class="toc-text">Foreword</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Graphs-and-the-isomorphism-problem"><span class="toc-number">2.</span> <span class="toc-text">Graphs and the isomorphism problem</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Graphs-fundamentals"><span class="toc-number">2.1.</span> <span class="toc-text">Graphs fundamentals</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Isomorphic-graphs"><span class="toc-number">2.2.</span> <span class="toc-text">Isomorphic graphs</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Permutation-invariance-of-GNNs"><span class="toc-number">2.3.</span> <span class="toc-text">Permutation invariance of GNNs</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Graph-autoencoders"><span class="toc-number">3.</span> <span class="toc-text">Graph autoencoders</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#General-idea"><span class="toc-number">3.1.</span> <span class="toc-text">General idea</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Training-loss"><span class="toc-number">3.2.</span> <span class="toc-text">Training loss</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Solution-approaches"><span class="toc-number">4.</span> <span class="toc-text">Solution approaches</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Node-level-embeddings"><span class="toc-number">4.1.</span> <span class="toc-text">Node-level embeddings</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Graph-matching"><span class="toc-number">4.2.</span> <span class="toc-text">Graph matching</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Heuristic-node-ordering"><span class="toc-number">4.3.</span> <span class="toc-text">Heuristic node ordering</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Discriminator-loss"><span class="toc-number">4.4.</span> <span class="toc-text">Discriminator loss</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Permuter-network"><span class="toc-number">4.5.</span> <span class="toc-text">Permuter network</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Closing-remarks"><span class="toc-number">5.</span> <span class="toc-text">Closing remarks</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#References"><span class="toc-number">6.</span> <span class="toc-text">References</span></a></li></ol>
    </div>
  </span>
</div>

    
    <div class="content index width mx-auto px2 my4">
        
        <article class="h-entry post" itemscope itemtype="http://schema.org/BlogPosting">
  <header>
    
    <a href="https://muetsch.io/graph-autoencoders-and-the-reconstruction-loss-problem.html" class="u-url">
        <h1 class="posttitle p-name" itemprop="name headline" property="headline">
            Graph Autoencoders and the Reconstruction Loss Problem
        </h1>
    </a>



    <div class="meta">
      <span class="author" itemprop="author" property="author" itemscope itemtype="http://schema.org/Person" vocab="http://schema.org/" typeof="Person">
        <span itemprop="name" property="name" class="p-author h-card">Ferdinand Mütsch</span>
      </span>
      
    <div class="postdate">
        <time datetime="2025-04-14T07:07:10.000Z" class="dt-published" itemprop="datePublished" property="datePublished">2025-04-14</time>
    </div>


      
    <div class="article-tag">
        <i class="fas fa-tag"></i>
        <a class="tag-link-link" href="/tags/ai/" rel="tag">ai</a>, <a class="tag-link-link" href="/tags/machine-learning/" rel="tag">machine-learning</a>
    </div>



      <!--
      <div style="margin-top: 30px">
        <a href="https://liberapay.com/muety/" target="_blank"
          style="background-image: none; text-decoration: none;"><img
            src="https://badges.fw-web.space/liberapay/receives/muety.svg?logo=liberapay&style=flat-square" alt="Liberapay"
            style="height: auto !important;width: auto !important;"></a>
      </div>
      -->
    </div>
  </header>
  
  <div class="content e-content" itemprop="articleBody" property="articleBody">
    <h1 id="Foreword"><a href="#Foreword" class="headerlink" title="Foreword"></a>Foreword</h1><p>This article is meant to give a brief introduction into a prominent problem in the context of generative graph neural network (GNN) models and a few examples of different approaches to defeat it. More precisely, we will look into variational autoencoders (VAE) and why the “reconstruction” part of their loss function is problematic when dealing with graphs.</p>
<p>Side note: if you want to learn more about the fundamentals of graph neural networks, I can highly recommend the following resources:</p>
<ul>
<li>📚 Graph Representation Learning, by Hamilton [2]</li>
<li>📚 Graph Neural Networks: Foundations, Frontiers, and Applications, by Wu et al. [1]</li>
<li>🎥 <a href="https://www.youtube.com/watch?v=JAB_plj2rbA&list=PLoROMvodv4rPLKxIpqhjhPgdQy7imNkDn">Stanford CS224W: Machine Learning with Graphs</a>, by Prof. Leskovec</li>
<li>📄 <a href="https://rish-16.github.io/posts/gnn-math/">Math Behind Graph Neural Networks</a> (2022)</li>
<li>📄 <a href="https://distill.pub/2021/gnn-intro/">A Gentle Introduction to Graph Neural Networks</a> (2021)</li>
</ul>
<h1 id="Graphs-and-the-isomorphism-problem"><a href="#Graphs-and-the-isomorphism-problem" class="headerlink" title="Graphs and the isomorphism problem"></a>Graphs and the isomorphism problem</h1><h2 id="Graphs-fundamentals"><a href="#Graphs-fundamentals" class="headerlink" title="Graphs fundamentals"></a>Graphs fundamentals</h2><p>A graph can generally be denoted as <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">G</mi><mo>=</mo><mo stretchy="false">(</mo><mrow><mi mathvariant="script">V</mi><mo separator="true">,</mo><mrow><mi mathvariant="script">E</mi><mo separator="true">,</mo><mrow><msub><mi mathvariant="script">X</mi><mi mathvariant="script">V</mi></msub><mo separator="true">,</mo><mrow><msub><mi mathvariant="script">X</mi><mi mathvariant="script">E</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\cal{G} = (\cal{V}, \cal{E}, \cal{X_V}, \cal{X_E})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord"><span class="mord mathcal" style="margin-right:0.0593em;">G</span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mopen">(</span><span class="mord"><span class="mord"><span class="mord mathcal" style="margin-right:0.08222em;">V</span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord"><span class="mord mathcal" style="margin-right:0.08944em;">E</span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord mathcal" style="margin-right:0.14643em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:-0.1464em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathcal mtight" style="margin-right:0.08222em;">V</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord mathcal" style="margin-right:0.14643em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:-0.1464em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathcal mtight" style="margin-right:0.08944em;">E</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span></span></span></span></span>, that is, a 4-tuple of nodes, edges and node- end edge attributes. The attributes, also called <em>features</em>, are optional and, if present, usually exist as <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">d</mi></mrow><annotation encoding="application/x-tex">\cal{d}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal">d</span></span></span></span></span></span>-dimensional vectors. A more complex variant of graphs are <em>heterogeneous</em> graphs, consisting of multiple different types of nodes and edges, however, we focus only on homogeneous graphs in the following. The excellent “GNN Book” [1] lists various other types in addition (page xxxi).</p>
<p>A graph’s <em>connectivity</em>, i.e. its edges, is usually represented by the <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>×</mo><mi>N</mi></mrow><annotation encoding="application/x-tex">N \times N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span></span> <em>adjacency matrix</em> <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">A</mi></mrow><annotation encoding="application/x-tex">\cal{A}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord"><span class="mord"><span class="mord mathcal">A</span></span></span></span></span></span> (N is the number of nodes) that has either a binary or continuous (between 0 and 1) value, depending on whether edges are discrete or probabilistic. </p>
<h2 id="Isomorphic-graphs"><a href="#Isomorphic-graphs" class="headerlink" title="Isomorphic graphs"></a>Isomorphic graphs</h2><p>While for images, you can say that <em>“pixel X is at position P”</em> or <em>“pixel X is right of pixel Y”</em>, graphs’ nodes do not have a fixed ordering and are not subject to any notion of spatial locality. There are many different ways to describe the same graph. For example, the graphs in fig. 1 “look” differently and have different adjacency matrices, but are structurally the same. More examples and a great interactive visualization can be found <a href="https://distill.pub/2021/gnn-intro/">here</a> (thanks!).</p>
<img src="images/gae_isomorphic_graphs.svg">
<center><small>Fig. 1: Two isomorphic graphs</small></center>

<p>For this reason, the adjacency matrix itself is not a suitable representation of a graph in the context of machine learning. Imagine we had a generative model (like an autoencoder) that is tasked to produce the first graph (see fig. 1), but instead returns the second. When comparing the adjacency matrices, the model appears to be way off the mark even though is actually perfectly reconstructed the given graph.</p>
<h2 id="Permutation-invariance-of-GNNs"><a href="#Permutation-invariance-of-GNNs" class="headerlink" title="Permutation invariance of GNNs"></a>Permutation invariance of GNNs</h2><blockquote>
<p>[…] graph-level representation should not depend on the order of the nodes in the input representation, i.e. two isomorphic graphs should always be mapped to the same representation. [9]</p>
</blockquote>
<p>Due to the above mentioned isomorphism problem, a crucial requirement for graph neural networks is to be <em>permutation-invariant</em> or, at least, <em>permutation-equivariant</em>. That is, regardless of the ordering of nodes in the adjacency matrix, the embedding produced by the GNNs must be the same for the same graph (or an isomorphic variant of it) – or, in the case of equivariance, at least be “shifted” in the same way the nodes were shifted.</p>
<p><img src="/images/gae_permutation_invariance.webp"></p>
<center>
<small>
Fig. 2: Examples of how permutation (permuted rows) affects different types of graph  operations. Left: Summation is invariant. Middle: Node-level MLP is equivariant. Right: An RNN with a positional history is neither. Credits: Felix Häusle, 2025.
</small>
</center>

<p>In formal terms:</p>
<ul>
<li><strong>Permutation invariance:</strong> <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">f</mi><mo stretchy="false">(</mo><mi mathvariant="script">P</mi><mrow><mi mathvariant="script">A</mi><mo stretchy="false">)</mo><mo>=</mo><mrow><mi mathvariant="script">f</mi><mo stretchy="false">(</mo><mrow><mi mathvariant="script">A</mi><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\cal{f}(P\cal{A}) = \cal{f}(\cal{A})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal" style="margin-right:0.10764em;">f</span></span><span class="mopen">(</span><span class="mord mathcal" style="margin-right:0.08222em;">P</span><span class="mord"><span class="mord"><span class="mord mathcal">A</span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal" style="margin-right:0.10764em;">f</span></span><span class="mopen">(</span><span class="mord"><span class="mord"><span class="mord mathcal">A</span></span><span class="mclose">)</span></span></span></span></span></span></span></span></li>
<li><strong>Permutation equivariance:</strong> <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">f</mi><mo stretchy="false">(</mo><mi mathvariant="script">P</mi><mrow><mi mathvariant="script">A</mi><mo stretchy="false">)</mo><mo>=</mo><mi mathvariant="script">P</mi><mrow><mi mathvariant="script">f</mi><mo stretchy="false">(</mo><mrow><mi mathvariant="script">A</mi><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\cal{f}(P\cal{A}) = P\cal{f}(\cal{A})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal" style="margin-right:0.10764em;">f</span></span><span class="mopen">(</span><span class="mord mathcal" style="margin-right:0.08222em;">P</span><span class="mord"><span class="mord"><span class="mord mathcal">A</span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mord mathcal" style="margin-right:0.08222em;">P</span><span class="mord"><span class="mord"><span class="mord mathnormal" style="margin-right:0.10764em;">f</span></span><span class="mopen">(</span><span class="mord"><span class="mord"><span class="mord mathcal">A</span></span><span class="mclose">)</span></span></span></span></span></span></span></span></li>
</ul>
<h1 id="Graph-autoencoders"><a href="#Graph-autoencoders" class="headerlink" title="Graph autoencoders"></a>Graph autoencoders</h1><h2 id="General-idea"><a href="#General-idea" class="headerlink" title="General idea"></a>General idea</h2><p>Autoencoders are one of the most popular self-supervised learning approaches in the context of deep generative models. The basic idea si to map an arbitrary input <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">x</span></span></span></span> to a latent representation <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>z</mi></mrow><annotation encoding="application/x-tex">z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span></span></span></span> (encoder) and train them to, subsequently, reconstruct <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>x</mi><mo>^</mo></mover></mrow><annotation encoding="application/x-tex">\hat{x}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6944em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal">x</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.2222em;"><span class="mord">^</span></span></span></span></span></span></span></span></span></span> from the latent. In case of <em>variational</em> autoencoders (VAE), the latent space <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Z</mi></mrow><annotation encoding="application/x-tex">Z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">Z</span></span></span></span> is forced to roughly follow some simple prior distribution, usually a standard normal. The encoder, in case of VAE, does not output a latent vector directly, but rather the parameters of this distribution, i.e. usually <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>μ</mi><mo separator="true">,</mo><mi>σ</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(\mu, \sigma)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathnormal">μ</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span><span class="mclose">)</span></span></span></span>. At inference time, we can then from this <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo stretchy="false">(</mo><mi>z</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">p(z)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="mclose">)</span></span></span></span> to obtain a conditioning <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>z</mi></mrow><annotation encoding="application/x-tex">z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span></span></span></span> for the decoder to generate a new sample. Formally, the encoder models <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>q</mi><mi>ϕ</mi></msub><mo stretchy="false">(</mo><mi>z</mi><mi mathvariant="normal">∣</mi><mrow><mi mathvariant="script">G</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">q_\phi(z | \cal{G})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0361em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">ϕ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="mord">∣</span><span class="mord"><span class="mord"><span class="mord mathcal" style="margin-right:0.0593em;">G</span></span><span class="mclose">)</span></span></span></span></span>, while the decoder models <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>p</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><mrow><mi mathvariant="script">G</mi><mi mathvariant="script">∣</mi><mi mathvariant="script">z</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">p_\theta(\cal{G} | z)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord"><span class="mord mathcal" style="margin-right:0.0593em;">G</span></span><span class="mord">∣</span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="mclose">)</span></span></span></span></span>.</p>
<p>Depending on the problem definition, autoencoders can be used to generate structure (<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">A</mi></mrow><annotation encoding="application/x-tex">\cal{A}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord"><span class="mord"><span class="mord mathcal">A</span></span></span></span></span></span>), features (<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">X</mi></mrow><annotation encoding="application/x-tex">\cal{X}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord"><span class="mord"><span class="mord mathcal" style="margin-right:0.14643em;">X</span></span></span></span></span></span>) or both. Also depending on the particular use case at hand, the autoencoder can either operate on a per-node level or generate entire graphs. In other words, the encoder produces either <strong>node-level latents</strong> or <strong>graph-level latents</strong> [2], illustrated in fig. 3. The problem addressed on this article only applies to the latter, i.e. the case where <em>entire</em> graphs are mapped to <em>single</em> embedding vectors (and reconstructed from such).</p>
<img src="images/gae_node_graph_level.svg" width="70%">
<center>
<small>
Fig. 3: Node-level latens (top) vs. graph-level latens (bottom)
</small>
</center>

<p>Fig. 4 shows an exemplary architecture of a graph autoencoder. In this case, structure <em>and</em> features are used as an input, however, only structural information is actually subject to generation. The decoder is realized as a simple dot-product, but could, alternatively, also consist of GNN- and &#x2F; or sequence model layers. The depicted architecture is limited to fixed-sized graphs of non-variable structure.</p>
<img src="images/vgae.svg" style="background: #fff; padding: 0 16px 16px 16px" width="100%">
<center>
<small>
Fig. 4: Architecture of VGAE [3]. Image credit: Felix Häusle, 2025. 
</small>
</center>

<h2 id="Training-loss"><a href="#Training-loss" class="headerlink" title="Training loss"></a>Training loss</h2><p>The problem considered in this article becomes clear when looking at the objective function with respect to which autoencoders are being trained. It’s called “ELBO” (evidence likelihood lower bound) and looks like so:</p>
<center>
<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">L</mi><mo stretchy="false">(</mo><mi mathvariant="script">θ</mi><mo separator="true">,</mo><mi mathvariant="script">ϕ</mi><mo separator="true">,</mo><mi mathvariant="script">x</mi><mo stretchy="false">)</mo><mo>=</mo><msub><mi mathvariant="double-struck">E</mi><mrow><msub><mi mathvariant="script">q</mi><mi mathvariant="script">ϕ</mi></msub><mo stretchy="false">(</mo><mi mathvariant="script">z</mi><mi mathvariant="script">∣</mi><mi mathvariant="script">x</mi><mo stretchy="false">)</mo></mrow></msub><mo stretchy="false">[</mo><mtext>log </mtext><msub><mi mathvariant="script">p</mi><mi mathvariant="script">θ</mi></msub><mo stretchy="false">(</mo><mi mathvariant="script">x</mi><mi mathvariant="script">∣</mi><mi mathvariant="script">z</mi><mo stretchy="false">)</mo><mo stretchy="false">]</mo><mo>−</mo><msub><mtext>D</mtext><mtext>KL</mtext></msub><mo stretchy="false">(</mo><msub><mi mathvariant="script">q</mi><mi mathvariant="script">ϕ</mi></msub><mo stretchy="false">(</mo><mi mathvariant="script">z</mi><mi mathvariant="script">∣</mi><mi mathvariant="script">x</mi><mo stretchy="false">)</mo><mtext> </mtext><mi mathvariant="script">∥</mi><mtext> </mtext><msub><mi mathvariant="script">p</mi><mi mathvariant="script">θ</mi></msub><mo stretchy="false">(</mo><mi mathvariant="script">z</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\cal{L}(\theta, \phi, x) = \mathbb{E}_{q_\phi(z|x)} \lbrack \text{log } p_\theta(x|z) \rbrack - \text{D}_\text{KL}(q_\phi(z|x) \text{ } \| \text{ } p_\theta(z))</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1333em;vertical-align:-0.3833em;"></span><span class="mord"><span class="mord"><span class="mord mathcal">L</span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">ϕ</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mord"><span class="mord mathbb">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.5198em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3488em;margin-left:-0.0359em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">ϕ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2901em;"><span></span></span></span></span></span></span><span class="mopen mtight">(</span><span class="mord mathnormal mtight" style="margin-right:0.04398em;">z</span><span class="mord mtight">∣</span><span class="mord mathnormal mtight">x</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3833em;"><span></span></span></span></span></span></span><span class="mopen">[</span><span class="mord text"><span class="mord">log </span></span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mord">∣</span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="mclose">)]</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord"><span class="mord text"><span class="mord">D</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">KL</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">ϕ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="mord">∣</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mord text"><span class="mord"> </span></span><span class="mord">∥</span><span class="mord text"><span class="mord"> </span></span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="mclose">))</span></span></span></span></span>
</center>

<center>
<small>Eq. 1: VAE ELBO loss</small>
</center>

<p>Do not worry too much about the actual equation. The key takeaway here is that it primarily consists of two parts. The <strong>regularization loss</strong> (right-hand of the minus) ensures that the latent space does not diverge too far from the chosen prior distribution. This part is not of importance here. The <strong>reconstruction loss</strong> (left of the minus), on the other hand, measures the degree to which the model output resembles the input.</p>
<p>And here comes the problem: as we learned earlier, there are a potentially a giant number (<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo stretchy="false">!</mo></mrow><annotation encoding="application/x-tex">n!</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">n</span><span class="mclose">!</span></span></span></span>) of different valid adjacency matrices for the exact same graph. If the model outputs some graph that is not the <em>same</em>, but <em>isomorphic</em> to the input, the reconstruction part of the loss might be maximally high (compare fig. 1), even though, in actual fact, the model had learned the right thing. Just like that, <strong>there is no obvious way to tell if the generated graph matches the input.</strong> Eq. 1 requires to specify a node ordering for <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi mathvariant="script">A</mi><mo>^</mo></mover></mrow><annotation encoding="application/x-tex">\hat{\cal{A}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9468em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9468em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathcal">A</span></span></span></span><span style="top:-3.2523em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.1111em;"><span class="mord">^</span></span></span></span></span></span></span></span></span></span> [2], however, when squashing the entire graph down to a single latent (usually through pooling layers), any explicit node ordering information got lost. Simply comparing <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">A</mi></mrow><annotation encoding="application/x-tex">\cal{A}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord"><span class="mord"><span class="mord mathcal">A</span></span></span></span></span></span> with <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi mathvariant="script">A</mi><mo>^</mo></mover></mrow><annotation encoding="application/x-tex">\hat{\cal{A}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9468em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9468em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathcal">A</span></span></span></span><span style="top:-3.2523em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.1111em;"><span class="mord">^</span></span></span></span></span></span></span></span></span></span> and &#x2F; or <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="script">X</mi><mi mathvariant="script">V</mi></msub></mrow><annotation encoding="application/x-tex">\cal{X_V}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord mathcal" style="margin-right:0.14643em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:-0.1464em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathcal mtight" style="margin-right:0.08222em;">V</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span></span> with <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><msub><mi mathvariant="script">X</mi><mi mathvariant="script">V</mi></msub><mo>^</mo></mover></mrow><annotation encoding="application/x-tex">\hat{\cal{X_V}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0968em;vertical-align:-0.15em;"></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9468em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord mathcal" style="margin-right:0.14643em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:-0.1464em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathcal mtight" style="margin-right:0.08222em;">V</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span><span style="top:-3.2523em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.25em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span> is not sufficient.</p>
<blockquote>
<p>[…] there is no straight-forward way to train an autoencoder network, due to the ambiguous reconstruction objective. [9]</p>
</blockquote>
<p>What we would want to have is either the decoder or the reconstruction loss calculation itself to be permutation-invariant somehow (permutation-invariance of the <em>encoder</em> is relatively easy to achieve when using GNN layers). Or, possibly, even get rid of it entirely or only <em>approximate</em> it in some way. Literature has different approaches to go about this.</p>
<h1 id="Solution-approaches"><a href="#Solution-approaches" class="headerlink" title="Solution approaches"></a>Solution approaches</h1><h2 id="Node-level-embeddings"><a href="#Node-level-embeddings" class="headerlink" title="Node-level embeddings"></a>Node-level embeddings</h2><p>As explained earlier, the whole problem only exists when dealing with graph-level latents in the first place, so if we can settle for node-level representations, this might be a legitimate way forward. VGAE [3] (see fig. 3) and ARGVA [4] essentially follow this approach. However, it comes with some limitations. Besides the fact of being unsuitable in cases where you explicitly <em>want</em> graph-level embeddings, akin architectures are usually limited to graph with a (small) fixed or maximum number of nodes. Also, the decoder can overfit to arbitrary node ordering during training [2], which hampers training success. Consequently, this is not actually a real <em>solution</em> to our problem at hand, but nevertheless worth being mentioned.</p>
<h2 id="Graph-matching"><a href="#Graph-matching" class="headerlink" title="Graph matching"></a>Graph matching</h2><p>An obvious way to achieve comparability of input and output node adjacency- and feature matrices is to align them using graph matching. <a href="https://en.wikipedia.org/wiki/Matching_(graph_theory)">Graph matching</a> is a well-known, established technique with the goal to determine the best bipartite mapping between nodes of two graphs, e.g. using a library like <a href="https://github.com/jacquesfize/GMatch4py">GMatch4py</a>. After the “best-matching” node ordering for some output graph has been found, it can be compared to the input for obtaining the reconstruction loss. GraphVAE [5] does exactly that. However, the drawback of this method (and related problems, like <a href="https://en.wikipedia.org/wiki/Graph_edit_distance">graph edit distance</a>) is its computational complexity (often times at least <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">O</mi><mo stretchy="false">(</mo><msup><mi mathvariant="script">V</mi><mn mathvariant="script">2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\cal{O}(V^2)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0641em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord"><span class="mord mathcal" style="margin-right:0.02778em;">O</span></span><span class="mopen">(</span><span class="mord"><span class="mord mathcal" style="margin-right:0.08222em;">V</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span>), rendering it inapplicable for larger graphs. More recently, graph matching networks (GMN) [6] have also emerged as an alternative to traditional approaches, the idea of which is to <em>predict</em> a good matching instead of analytically computing one.</p>
<h2 id="Heuristic-node-ordering"><a href="#Heuristic-node-ordering" class="headerlink" title="Heuristic node ordering"></a>Heuristic node ordering</h2><p>Another strategy is to simply enforce a mapping based on some simple heuristics. For example, one could choose to order nodes on a depth-first or breadth-first search, starting from the highest-degree node [2]. More details and a comparison of different approaches is provided by Liao et al. [7].</p>
<h2 id="Discriminator-loss"><a href="#Discriminator-loss" class="headerlink" title="Discriminator loss"></a>Discriminator loss</h2><p><strong>CondGen</strong> [8] tackles the problem by essentially replacing the reconstruction loss with a discriminator loss. Inspired by architecture of generative adversarial networks (GAN), they train a discriminator network in addition the encoder and decoder that is tasked to distinguish between real and generated data samples. During training, it inherently learns to map isomorphic graph structures to identical (or at least very similar) latent vectors in a permutation-invariant manner. The reconstruction loss is then simply computed as the Euclidean distance between the discriminator’s latent embeddings of <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">A</mi></mrow><annotation encoding="application/x-tex">\cal{A}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord"><span class="mord"><span class="mord mathcal">A</span></span></span></span></span></span> and <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi mathvariant="script">A</mi><mo>^</mo></mover></mrow><annotation encoding="application/x-tex">\hat{\cal{A}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9468em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9468em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathcal">A</span></span></span></span><span style="top:-3.2523em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.1111em;"><span class="mord">^</span></span></span></span></span></span></span></span></span></span>. In some sense, what CondGen proposes is a VAE architecture “wrapped” by a GAN architecture.</p>
<center>
<img src="images/condgen.webp" width="80%">
</center>
<center>
<small>Fig. 5: CondGen architecture [8]</small>
</center>

<h2 id="Permuter-network"><a href="#Permuter-network" class="headerlink" title="Permuter network"></a>Permuter network</h2><p>Even another interesting (though fairly involved) technique is presented by <strong>PIGVAE</strong> [9]. They introduce the concept of a “permuter network”, which is trained alongside the encoder and decoder with the aim to learn the decoder’s “canonical” ordering. It allows to derive a permutation matrix for each input graph, which is ultimately used to align input and output graphs. More precisely, the permuter predicts scalar “scores” for each node (mapping from <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>N</mi><mo>×</mo><mi>D</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(N \times D)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="mclose">)</span></span></span></span> to <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>N</mi><mo>×</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(N \times 1)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">1</span><span class="mclose">)</span></span></span></span>), which, in some sense, correspond to the probability of the respective node to have a low node index in the decoded graph’s ordering. This score vector, encoded using some positional encoding function, is then fed to the decoder as a conditioning in order to guide it to deviate from its inherent canonical ordering and instead produce an output ordering that matches the input. In other words: given the input graph’s embedding, the idea is to anticipate the decoder’s output ordering, derive a permutation matrix to reorder it in the desired way and pass that (in encoded form) to the decoder (alongside the actual <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>z</mi></mrow><annotation encoding="application/x-tex">z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span></span></span></span>) to actually apply it. So much more the big picture, the while concept and implementation are even a lot more elaborate.</p>
<center>
<img src="images/pigvae.webp" width="80%">
</center>
<center>
<small>Fig. 6: PIGVAE architecture [9]</small>
</center>

<h1 id="Closing-remarks"><a href="#Closing-remarks" class="headerlink" title="Closing remarks"></a>Closing remarks</h1><p>I hope to have given a somewhat useful overview for one of the most prominent challenges with graph autoencoders. Note that all of this is only relevant for <em>generative</em> tasks. If your problem at hand is of discriminative nature instead (traditional representation learning), e.g. wanting to compute graph embeddings for similarity learning, you will probably not need autoencoders, but go with simpler approaches like <strong>contrastive learning</strong> instead.</p>
<p>Disclaimer: all of the above is based on my own understanding and provided without guarantee.</p>
<h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><p>[1] L. Wu, P. Cui, J. Pei, and L. Zhao, Eds., Graph Neural Networks: Foundations, Frontiers, and Applications. Singapore: Springer Nature, 2022. Online at <a href="https://graph-neural-networks.github.io/">https://graph-neural-networks.github.io/</a>.<br>[2] W. L. Hamilton, Graph Representation Learning. in Synthesis Lectures on Artificial Intelligence and Machine Learning. Cham: Springer International Publishing, 2020.<br>[3] T. N. Kipf and M. Welling, “Variational Graph Auto-Encoders,” Nov. 21, 2016, arXiv: arXiv:1611.07308.<br>[4] S. Pan, R. Hu, G. Long, J. Jiang, L. Yao, and C. Zhang, “Adversarially Regularized Graph Autoencoder for Graph Embedding,” Jan. 07, 2019, arXiv: arXiv:1802.04407. doi: 10.48550&#x2F;arXiv.1802.04407.<br>[5] M. Simonovsky and N. Komodakis, “GraphVAE: Towards Generation of Small Graphs Using Variational Autoencoders,” in Artificial Neural Networks and Machine Learning – ICANN 2018, V. Kůrková, Y. Manolopoulos, B. Hammer, L. Iliadis, and I. Maglogiannis, Eds., Cham: Springer International Publishing, 2018, pp. 412–422.<br>[6] Y. Li, C. Gu, T. Dullien, O. Vinyals, and P. Kohli, “Graph Matching Networks for Learning the Similarity of Graph Structured Objects,” in Proceedings of the 36th International Conference on Machine Learning, PMLR, May 2019, pp. 3835–3845.<br>[7] R. Liao et al., “Efficient Graph Generation with Graph Recurrent Attention Networks,” in Advances in Neural Information Processing Systems, Curran Associates, Inc., 2019.<br>[8] C. Yang, P. Zhuang, W. Shi, A. Luu, and P. Li, “Conditional Structure Generation through Graph Variational Generative Adversarial Nets,” in Advances in Neural Information Processing Systems, Curran Associates, Inc., 2019.<br>[9] R. Winter, F. Noe, and D.-A. Clevert, “Permutation-Invariant Variational Autoencoder for Graph-Level Representation Learning,” presented at the Advances in Neural Information Processing Systems, Nov. 2021.</p>

  </div>
</article>



    </div>
    
      <div id="footer-post-container">
  <div id="footer-post">

    <div id="nav-footer" style="display: none">
      <ul>
         
          <li><a href="/">Home</a></li>
         
          <li><a href="/about/">About</a></li>
         
          <li><a href="/archives/">Blog</a></li>
         
          <li><a href="/reads/">Reads</a></li>
         
          <li><a href="/imprint/">Imprint</a></li>
        
      </ul>
    </div>

    <div id="toc-footer" style="display: none">
      <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Foreword"><span class="toc-number">1.</span> <span class="toc-text">Foreword</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Graphs-and-the-isomorphism-problem"><span class="toc-number">2.</span> <span class="toc-text">Graphs and the isomorphism problem</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Graphs-fundamentals"><span class="toc-number">2.1.</span> <span class="toc-text">Graphs fundamentals</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Isomorphic-graphs"><span class="toc-number">2.2.</span> <span class="toc-text">Isomorphic graphs</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Permutation-invariance-of-GNNs"><span class="toc-number">2.3.</span> <span class="toc-text">Permutation invariance of GNNs</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Graph-autoencoders"><span class="toc-number">3.</span> <span class="toc-text">Graph autoencoders</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#General-idea"><span class="toc-number">3.1.</span> <span class="toc-text">General idea</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Training-loss"><span class="toc-number">3.2.</span> <span class="toc-text">Training loss</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Solution-approaches"><span class="toc-number">4.</span> <span class="toc-text">Solution approaches</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Node-level-embeddings"><span class="toc-number">4.1.</span> <span class="toc-text">Node-level embeddings</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Graph-matching"><span class="toc-number">4.2.</span> <span class="toc-text">Graph matching</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Heuristic-node-ordering"><span class="toc-number">4.3.</span> <span class="toc-text">Heuristic node ordering</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Discriminator-loss"><span class="toc-number">4.4.</span> <span class="toc-text">Discriminator loss</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Permuter-network"><span class="toc-number">4.5.</span> <span class="toc-text">Permuter network</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Closing-remarks"><span class="toc-number">5.</span> <span class="toc-text">Closing remarks</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#References"><span class="toc-number">6.</span> <span class="toc-text">References</span></a></li></ol>
    </div>

    <div id="share-footer" style="display: none">
      <ul>
  <li><a class="icon" href="http://www.facebook.com/sharer.php?u=https://muetsch.io/graph-autoencoders-and-the-reconstruction-loss-problem.html"><i class="fas fa-facebook fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://twitter.com/share?url=https://muetsch.io/graph-autoencoders-and-the-reconstruction-loss-problem.html&text=Graph Autoencoders and the Reconstruction Loss Problem"><i class="fas fa-twitter fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.linkedin.com/shareArticle?url=https://muetsch.io/graph-autoencoders-and-the-reconstruction-loss-problem.html&title=Graph Autoencoders and the Reconstruction Loss Problem"><i class="fas fa-linkedin fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://pinterest.com/pin/create/bookmarklet/?url=https://muetsch.io/graph-autoencoders-and-the-reconstruction-loss-problem.html&is_video=false&description=Graph Autoencoders and the Reconstruction Loss Problem"><i class="fas fa-pinterest fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=Graph Autoencoders and the Reconstruction Loss Problem&body=Check out this article: https://muetsch.io/graph-autoencoders-and-the-reconstruction-loss-problem.html"><i class="fas fa-envelope fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://getpocket.com/save?url=https://muetsch.io/graph-autoencoders-and-the-reconstruction-loss-problem.html&title=Graph Autoencoders and the Reconstruction Loss Problem"><i class="fas fa-get-pocket fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://reddit.com/submit?url=https://muetsch.io/graph-autoencoders-and-the-reconstruction-loss-problem.html&title=Graph Autoencoders and the Reconstruction Loss Problem"><i class="fas fa-reddit fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.stumbleupon.com/submit?url=https://muetsch.io/graph-autoencoders-and-the-reconstruction-loss-problem.html&title=Graph Autoencoders and the Reconstruction Loss Problem"><i class="fas fa-stumbleupon fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://digg.com/submit?url=https://muetsch.io/graph-autoencoders-and-the-reconstruction-loss-problem.html&title=Graph Autoencoders and the Reconstruction Loss Problem"><i class="fas fa-digg fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.tumblr.com/share/link?url=https://muetsch.io/graph-autoencoders-and-the-reconstruction-loss-problem.html&name=Graph Autoencoders and the Reconstruction Loss Problem&description="><i class="fas fa-tumblr fa-lg" aria-hidden="true"></i></a></li>
</ul>

    </div>

    <div id="actions-footer">
      <ul>
        <li id="toc"><a class="icon" href="#" onclick='toggleById("toc-footer")'><i class="fas fa-list fa-lg" aria-hidden="true"></i> TOC</a></li>
        <li id="share"><a class="icon" href="#" onclick='toggleById("share-footer")'><i class="fas fa-share-alt fa-lg" aria-hidden="true"></i> Share</a></li>
        <li id="top" style="display:none"><a class="icon" href="#" onclick="window.scrollTo(0,0)"><i class="fas fa-chevron-up fa-lg" aria-hidden="true"></i> Top</a></li>
        <li id="menu"><a class="icon" href="#" onclick='toggleById("nav-footer")'><i class="fas fa-bars fa-lg" aria-hidden="true"></i> Menu</a></li>
      </ul>
    </div>

  </div>
</div>

    
    <footer id="footer">
  <div class="footer-left">
    Copyright &copy; 2025 Ferdinand Mütsch
  </div>
  <div class="footer-right">
    <nav>
      <ul>
         
          <li><a href="/">Home</a></li>
         
          <li><a href="/about/">About</a></li>
         
          <li><a href="/archives/">Blog</a></li>
         
          <li><a href="/reads/">Reads</a></li>
         
          <li><a href="/imprint/">Imprint</a></li>
        
      </ul>
    </nav>
  </div>
</footer>

<!--
<script defer type="text/javascript" src="js/pirsch.js"
    id="pirschjs"
    data-code="GoPk8VNmzk8Z6n7xikwZahvOxm1MNPud"></script>
-->
</body>
</html>
<!-- styles -->

<link rel="stylesheet" href="/lib/roboto/css/roboto.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">




<script src="/js/main.js"></script>

<!-- Google Analytics -->

<!-- Disqus Comments -->


